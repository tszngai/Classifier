\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Exploring Different Approaches to Improve Binary Classification Performance for Imbalanced Data Set}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Yuan Sun\\
  Department of Computer Engineering\\
  University of British Columbia\\
  Vancouver, BC\\
  \texttt{round.sun@alumni.ubc.ca}\\
  \And
  Yixuan Ji\\
  Department of Computer Engineering\\
  University of British Columbia\\
  Vancouver, BC\\
  \texttt{jiyixuan@ece.ubc.ca}\\
  \AND
  Jay Fu\\
  Department of Computer Engineering\\
  University of British Columbia\\
  Vancouver, BC\\
  \texttt{jay.fu@alumni.ubc.ca}\\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  This is a good abstract.
\end{abstract}

\section{Introduction}

This is instruction section.

\subsection{sub}

This is a good sub section.

\subsection{sub}

This is a good sub section.

\section{Related Work}
\label{related_work}

This is related work section.

\section{Descriptions and Justifications}
\label{headings}

To boost up the accuracy of a model and minimize the effect of imbalanced data on the performance, there are several possible solutions, including undersampling, oversampling and class weighting. However, seldom have studied the difference in these approaches. The motivation of this experiment is to study how different machine learning techniques could have impact on the performance of models trained on imbalanced data set. The following sections discuss in detail the settings of the experiment and some of the approaches to tackle imbalanced problems.

\subsection{Data Set Settings}
The data set is drawn from the UCI Machine Learning Repository and is publicly known as the $Adult$ data set. It contains general information of 48843 individuals and whether or not they make more than 50K every year. The goal is to build a model that can accurately predict the annual income ($>50K$ or $<=50K$) of a given person based on this data set. As shown in Table~\ref{attributes}, the data set contains a mixture of categorical and numerical features for each entry. Therefore, data pre-processing techniques should be applied to enable further study on the data. Specifically, feature selecion algorithms could help decide which features are relevant to the prediction. In addition, the label $salary$ is a binary attribute of the individual's income being either $>50K$ or $<=50K$, which makes it an ideal binary classification problems.\\\\
The major challenge for this data set is the imbalance of its binary label. There are only 11687 positive ($>50K$) labels out of 48843 entries in total, which makes up $23.9\%$ of the whole data set. Models trained on imbalanced data set tend to make prediction of the majority class. For example, consider a data set consisting $10000$ entries of class $A$ and $100$ entries of class $B$. Then the model could get 90\% of training accuracy by simply predicting everything as class $A$. The following sections discuss several methods to handle imbalanced data set, including previous efforts that has been made to tackle this problem in particular, as well as additional machne learning techniques that could also be applied.

\begin{table}
  \caption{Data Set Attributes}
  \label{attributes}
  \centering
  \begin{tabular}{ll}
    \toprule
    Name     & Description\\
    \midrule
    Age & Continuous\\
    Workclass & Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov,\\ &State-gov, Without-pay, Never-worked\\
    Final Weight & The number of people the census believes the entry represents; Continuous\\
    Education & Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, \\&Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, \\&Preschool\\
    Education-Num & Continuous\\
    Marital-Status & Married-civ-spouse, Divorced, Never-married, Separated, Widowed, \\&Married-spouse-absent, Married-AF-spouse\\
    Occupation & Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, \\&Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, \\&Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, \\&Armed-Forces\\
    Relationship & Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\\
    Race & White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\\
    Sex & Female, Male\\
    Capital-Gain & Continuous\\
    Capital-Loss & Continuous\\
    Hours-per-Week & Continuous\\
    Native-Country & United-States, Cambodia, England, Puerto-Rico, Canada, Germany, \\&Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, \\&Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, \\&Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, \\&Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, \\&Yugoslavia, El-Salvador, Trinadad\& Tobago, Peru, Hong, \\&Holand-Netherlands\\
    Salary & >50K, <=50K\\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Undersampling}
\label{undersampling}

One of the most popular methods to handle imbalanced data sets is data undersampling, in which entries are randomly sampled from the majority class. Only a portion of the majority class are collected such that the size of the sampled majority class is the same as the minority class. Inouye (2018) has implemented such sampling method to the $Adult$ data set to randomly generate a subset of sample with income $<=50K$ and simply discarded the rest. This approach usually works relatively well, however, massive amount of data is discarded and the resulted model would not be able to reflect the comlete data set. Next section introduces an alternative method that could fully utilize every entry of the data set to build the model.

\subsection{Oversampling}
\label{oversampling}

As an alternative to the undersampling method, oversampling "creates" new entries of the minority class to match the size of majority class. The simplest way to oversample is to duplicate the minority class multiple times. There are, however, more sophisticated oversampling algorithms, such as SMOTE and ADASYN, that can create synthetic data points as oppose to duplicating original entries. The particular algorithm used in this experiment is ADASYN, which randomly generates a data point based on $k$ nearest neighbors with more weights are attributed to data points that are harder to learn. Both undersampling and oversampling are types of data pre-processing methods solve imbalance problems. Next section introduces another technique that could be applied in the training phase of model fitting.

\subsection{Class Weighting}
\label{class_weighting}

Class weighting is a method that assigns more weights on samples with important classes so that the model focuses more on one class than the other during training. This method can also be applied to imbalanced data set so the model would focus more on the minority class. Lo, et al. (2008) applied Class-Balanced SVM (CB-SVM) to solve imbalanced problems, in which different weights are assigned to each class as an effort of preventing the model from favoring majority class. Similarly, class weighting is also conducted in this experiment using the $svm$ module in $scikit-learn$ package. The specific function used, $SVC$, provides interface that allows users to specify weights for each class through the parameter $class\_weight$, which has greatly facilitated this experiment.

\subsection{Ensemble}
\label{ensemble}

Another approach that could potentially enhance the performance is model ensembling. In general it could help reduce the training error and/or approximation error of problems in a variety of domains, rather than just imbalanced data set. However, seldom have applied ensemble method to the $adult$ data set. In this experiment, ensemble methods are used as an attempt to reduce false predictions of the model. Specifically, stacking ensemble method is used in which outputs of a series of heterogeneous machine learning models are gathered as inputs to train a logistic regression model. Predictions are made from the output of this stacking model. Section~\ref{experiments} discusses in detail how different techniques, including ensemble methods, would impact on the performance.

\section{Experiments}
\label{experiments}

This is experiments section.

\subsection{sub}

This is a good sub section.

\subsection{sub}

This is a good sub section.

\subsection{sub}

This is a good sub section.

\subsection{sub}

This is a good sub section.

\section{Discussion}

This is discussion section.

\subsection{sub}

This is a good sub section.

\section*{References}

References follow the acknowledgments. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references. {\bf Remember that you can use more than eight
  pages as long as the additional pages contain \emph{only} cited references.}
\medskip

\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
